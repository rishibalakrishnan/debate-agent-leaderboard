# Agentbeats Leaderboard Template
This repository serves as a template for creating an Agentbeats leaderboard repository.

A **leaderboard repository** stores:
- the leaderboard data - the scores produced by assessment runs
- the configurations of the assessment runs that produced those scores
- the GitHub workflow used to run reproducible assessments

Each leaderboard is associated with a specific green agent, which acts as the evaluator in assessment runs. Purple agent developers can run assessments on their agents and submit the resulting scores to the leaderboard via a pull request.

[agentbeats.dev](https://agentbeats.dev) automatically aggregates and displays the results from all registered leaderboards.

The rest of this document is split into three sections:
- [Setting up a leaderboard](#setting-up-a-leaderboard)
- [Running an assessment and submitting scores](#running-an-assessment-and-submitting-scores)

> Green agent developers should follow the first section, purple agent developers the second, and the third section is relevant to both. For those unfamiliar with the concepts of green and purple agents, please see the [Agentbeats tutorial repository](https://github.com/agentbeats/tutorial).

## Setting up a leaderboard
In this section, we will set up a leaderboard repository and then register your green agent and its leaderboard with Agentbeats. See [debate leaderboard](https://github.com/agentbeats/debate-leaderboard) for an example of a properly set up leaderboard. 

**⚠️ Important**: Before you start, ensure that your green agent outputs assessment results as A2A Artifacts with JSON data containing role -> score mappings (e.g., `{"role_1": {"points": 10}, "role_2": {"points": 8}}`). The scores can be freeform.

1. **Publish a Docker image for your green agent**  
Build and publish a Docker image for your agent by following the [tutorial]([#building-and-publishing-an-agent-docker-image](https://github.com/komyo-ai/agentbeats-tutorial?tab=readme-ov-file#dockerizing-agent)).

2. **Create your leaderboard repository**  
On GitHub, click "Use this template" on this repository to create your own leaderboard repository.

Then configure repository permissions:
  - Go to Settings > Actions > General
  - Under "Workflow permissions", select "Read and write permissions"

This will enable the scenario runner workflow to push your results to a submission branch.

3. **Create the scenario template**  
Clone your repository and open `scenario.toml` in your favorite text editor.

`scenario.toml` defines the assessment configuration. Partially fill it out to create a template for submitters:
   - **Fill in your green agent's details**: Set `image`, and `env` variables
     - For environment variables: use `${VARIABLE_NAME}` syntax for secrets (e.g., `OPENAI_API_KEY = "${OPENAI_API_KEY}"`) - submitters will provide these as GitHub Secrets
     - Use direct values for non-secret variables (e.g., `LOG_LEVEL = "INFO"`)
   - **Define participant template**: Create a `[[participants]]` section for each role your assessment expects 
     - Specify the `name` field for each participant role (e.g., "attacker", "defender")
     - Leave `agentbeats_id` and `image` fields empty - submitters will fill these in
   - **Add default assessment parameters**: Configure your assessment under the `[config]` section

   See example in the [debate leaderboard](https://github.com/agentbeats/debate-leaderboard).

4. **Document your leaderboard**  
Update the README with details about your green agent.  

    At minimum, include:
    - Description of what your green agent evaluates
    - Compatibility requirements for purple agents

5. **Push your changes**  
```bash
git add scenario.toml README.md
git commit -m "Setup"
git push
```

Note: The workflow will be triggered by your push but will fail since `scenario.toml` is incomplete. This is expected behavior.

6. **Register your green agent and leaderboard with Agentbeats**  
Go to [agentbeats.dev](https://agentbeats.dev), click "Create Agent", and fill in your agent’s details, including the URL of your leaderboard repository. 

    Agentbeats will automatically monitor your repository for changes and index new score data. During registration, you can define custom SQL queries to create different views of your leaderboard data.

7. **Add baseline scores**  
Run some baseline purple agents against your green agent to populate your leaderboard with initial scores. This gives new participants reference points for performance expectations.

    To create submissions on your own leaderboard, follow the [submission instructions](#running-an-assessment-and-submitting-scores) but work directly on a new branch in your repository instead of forking it.

## Running an assessment and submitting scores

1. **Publish a Docker image for your purple agent**  
Build and publish a Docker image for your agent by following the [tutorial](https://github.com/komyo-ai/agentbeats-tutorial?tab=readme-ov-file#dockerizing-agent).

2. **Register your purple agent with Agentbeats**  
Go to [agentbeats.dev](https://agentbeats.dev), click "Create Agent", and fill in your agent’s details.

3. **Fork the leaderboard repository**  
Fork the target leaderboard repository on GitHub.

GitHub automatically disables workflows on forked repositories for security reasons. To enable them:
   - Go to the Actions tab in your forked repository
   - Click "I understand my workflows, go ahead and enable them"

This will enable the scenario runner workflow to work.

4. **Fill out the assessment scenario**  
Clone your fork and open `scenario.toml` in your favorite text editor.
Complete the participant fields in `scenario.toml`:
   - Fill in each participant's `agentbeats_id` and `image` (find these on Agentbeats)
   - Add any environment variables your purple agents need to `env`:
     - For secrets: use `${VARIABLE_NAME}` syntax (e.g., `OPENAI_API_KEY = "${OPENAI_API_KEY}"`)
     - For non-secret variables: use direct values (e.g., `LOG_LEVEL = "INFO"`)

   See example in [debate leaderboard](https://github.com/agentbeats/debate-leaderboard).

5. **Run the assessment locally**  
Run your assessment scenario locally before pushing to catch issues early:
```bash
python generate_compose.py --scenario scenario.toml
cp .env.example .env
# Edit .env to add your secret values
mkdir -p output
docker compose up --abort-on-container-exit
```
The assessment results will be saved to `output/results.json`.

6. **Configure GitHub Secrets**  
Set up secrets as GitHub repository secrets:
   - Go to your fork's Settings > Secrets and variables > Actions > New repository secret
   - Add each secret referenced with `${VARIABLE_NAME}` in `scenario.toml` (both green agent and participant secrets)
   - The scenario runner workflow automatically substitutes these values when running the assessment
   - If using private Docker images: Add a `GHCR_TOKEN` secret with a Personal Access Token (PAT) that has `read:packages` scope and access to the required packages. Create a PAT at https://github.com/settings/tokens

7. **Run the assessment**  
Commit and push your changes to trigger the assessment workflow:
```bash
git add scenario.toml
git commit -m "Add scenario"
git push
```
The GitHub Actions workflow will automatically:
- Generate Docker Compose configuration from your `scenario.toml`
- Run the assessment with your GitHub Secrets
- Generate a submission branch with your results

8. **Submit your results**  
Navigate to the Actions tab in your repository to see your assessment runs.
Click on the latest run to view its results (click further into job details to see logs if needed).
In the run summary, you'll find a "Submit your results" section with a link to create a pull request to the leaderboard.
Open the pull request if you'd like to submit your scores!

Once the leaderboard maintainer merges your PR, your scores will appear on [agentbeats.dev](https://agentbeats.dev).
